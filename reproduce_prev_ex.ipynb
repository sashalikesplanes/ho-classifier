{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "47e24da4",
   "metadata": {
    "gradient": {
     "editing": false,
     "id": "5c065e38-f3ab-4302-b63e-b1eb7bd47673",
     "kernelId": "e731389a-9397-4fee-9c6b-efa8de72a05c"
    }
   },
   "source": [
    "### Reproduction of Inception Time algo applied to Fofu dataset from paper by Verket"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "900aafb7",
   "metadata": {},
   "source": [
    "Run cells to set github automatically"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8bf089e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "!git config --global user.email \"a.kiselev@student.tudelft.nl\"\n",
    "!git config --global user.name \"Alexander Kiselev\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a76326c7",
   "metadata": {},
   "source": [
    "Calls to install on the needed packages as the environment gets wiped every time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f9645dc9",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://pypi.ngc.nvidia.com\n",
      "Collecting llvmlite==0.38.0\n",
      "  Downloading llvmlite-0.38.0-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (34.5 MB)\n",
      "\u001b[K     |████████████████████████████████| 34.5 MB 24.6 MB/s eta 0:00:01\n",
      "\u001b[?25hInstalling collected packages: llvmlite\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "numba 0.52.0 requires llvmlite<0.36,>=0.35.0, but you have llvmlite 0.38.0 which is incompatible.\u001b[0m\n",
      "Successfully installed llvmlite-0.38.0\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n",
      "Looking in indexes: https://pypi.org/simple, https://pypi.ngc.nvidia.com\n",
      "Requirement already satisfied: sktime==0.9.0 in /opt/conda/lib/python3.8/site-packages (0.9.0)\n",
      "Requirement already satisfied: pandas>=1.1.0 in /opt/conda/lib/python3.8/site-packages (from sktime==0.9.0) (1.4.1)\n",
      "Requirement already satisfied: scikit-learn>=0.24.0 in /opt/conda/lib/python3.8/site-packages (from sktime==0.9.0) (1.0.2)\n",
      "Requirement already satisfied: deprecated>=1.2.13 in /opt/conda/lib/python3.8/site-packages (from sktime==0.9.0) (1.2.13)\n",
      "Requirement already satisfied: statsmodels<=0.12.1 in /opt/conda/lib/python3.8/site-packages (from sktime==0.9.0) (0.12.1)\n",
      "Requirement already satisfied: numba>=0.53 in /opt/conda/lib/python3.8/site-packages (from sktime==0.9.0) (0.54.1)\n",
      "Requirement already satisfied: numpy<=1.19.3 in /opt/conda/lib/python3.8/site-packages (from sktime==0.9.0) (1.19.3)\n",
      "Requirement already satisfied: wheel in /opt/conda/lib/python3.8/site-packages (from sktime==0.9.0) (0.37.0)\n",
      "Requirement already satisfied: wrapt<2,>=1.10 in /opt/conda/lib/python3.8/site-packages (from deprecated>=1.2.13->sktime==0.9.0) (1.13.3)\n",
      "Requirement already satisfied: llvmlite<0.38,>=0.37.0rc1 in /opt/conda/lib/python3.8/site-packages (from numba>=0.53->sktime==0.9.0) (0.37.0)\n",
      "Requirement already satisfied: setuptools in /opt/conda/lib/python3.8/site-packages (from numba>=0.53->sktime==0.9.0) (58.2.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in /opt/conda/lib/python3.8/site-packages (from pandas>=1.1.0->sktime==0.9.0) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.8/site-packages (from pandas>=1.1.0->sktime==0.9.0) (2021.3)\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.8/site-packages (from python-dateutil>=2.8.1->pandas>=1.1.0->sktime==0.9.0) (1.16.0)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /opt/conda/lib/python3.8/site-packages (from scikit-learn>=0.24.0->sktime==0.9.0) (3.0.0)\n",
      "Requirement already satisfied: scipy>=1.1.0 in /opt/conda/lib/python3.8/site-packages (from scikit-learn>=0.24.0->sktime==0.9.0) (1.6.3)\n",
      "Requirement already satisfied: joblib>=0.11 in /opt/conda/lib/python3.8/site-packages (from scikit-learn>=0.24.0->sktime==0.9.0) (1.1.0)\n",
      "Requirement already satisfied: patsy>=0.5 in /opt/conda/lib/python3.8/site-packages (from statsmodels<=0.12.1->sktime==0.9.0) (0.5.2)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n",
      "Looking in indexes: https://pypi.org/simple, https://pypi.ngc.nvidia.com\n",
      "Collecting numpy==1.20\n",
      "  Downloading numpy-1.20.0-cp38-cp38-manylinux2010_x86_64.whl (15.4 MB)\n",
      "\u001b[K     |████████████████████████████████| 15.4 MB 13.6 MB/s eta 0:00:01\n",
      "\u001b[?25hInstalling collected packages: numpy\n",
      "  Attempting uninstall: numpy\n",
      "    Found existing installation: numpy 1.19.3\n",
      "    Uninstalling numpy-1.19.3:\n",
      "      Successfully uninstalled numpy-1.19.3\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "sktime 0.9.0 requires numpy<=1.19.3, but you have numpy 1.20.0 which is incompatible.\u001b[0m\n",
      "Successfully installed numpy-1.20.0\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n",
      "Looking in indexes: https://pypi.org/simple, https://pypi.ngc.nvidia.com\n",
      "Collecting wandb\n",
      "  Downloading wandb-0.12.10-py2.py3-none-any.whl (1.7 MB)\n",
      "\u001b[K     |████████████████████████████████| 1.7 MB 18.1 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting GitPython>=1.0.0\n",
      "  Downloading GitPython-3.1.27-py3-none-any.whl (181 kB)\n",
      "\u001b[K     |████████████████████████████████| 181 kB 28.9 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting docker-pycreds>=0.4.0\n",
      "  Downloading docker_pycreds-0.4.0-py2.py3-none-any.whl (9.0 kB)\n",
      "Collecting promise<3,>=2.0\n",
      "  Downloading promise-2.3.tar.gz (19 kB)\n",
      "Requirement already satisfied: Click!=8.0.0,>=7.0 in /opt/conda/lib/python3.8/site-packages (from wandb) (8.0.1)\n",
      "Collecting pathtools\n",
      "  Downloading pathtools-0.1.2.tar.gz (11 kB)\n",
      "Collecting protobuf>=3.12.0\n",
      "  Downloading protobuf-3.19.4-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.1 MB)\n",
      "\u001b[K     |████████████████████████████████| 1.1 MB 31.5 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting sentry-sdk>=1.0.0\n",
      "  Downloading sentry_sdk-1.5.5-py2.py3-none-any.whl (144 kB)\n",
      "\u001b[K     |████████████████████████████████| 144 kB 33.9 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting yaspin>=1.0.0\n",
      "  Downloading yaspin-2.1.0-py3-none-any.whl (18 kB)\n",
      "Collecting shortuuid>=0.5.0\n",
      "  Downloading shortuuid-1.0.8-py3-none-any.whl (9.5 kB)\n",
      "Requirement already satisfied: six>=1.13.0 in /opt/conda/lib/python3.8/site-packages (from wandb) (1.16.0)\n",
      "Requirement already satisfied: psutil>=5.0.0 in /opt/conda/lib/python3.8/site-packages (from wandb) (5.8.0)\n",
      "Requirement already satisfied: requests<3,>=2.0.0 in /opt/conda/lib/python3.8/site-packages (from wandb) (2.26.0)\n",
      "Requirement already satisfied: python-dateutil>=2.6.1 in /opt/conda/lib/python3.8/site-packages (from wandb) (2.8.2)\n",
      "Requirement already satisfied: PyYAML in /opt/conda/lib/python3.8/site-packages (from wandb) (5.4.1)\n",
      "Collecting gitdb<5,>=4.0.1\n",
      "  Downloading gitdb-4.0.9-py3-none-any.whl (63 kB)\n",
      "\u001b[K     |████████████████████████████████| 63 kB 38.9 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting smmap<6,>=3.0.1\n",
      "  Downloading smmap-5.0.0-py3-none-any.whl (24 kB)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.8/site-packages (from requests<3,>=2.0.0->wandb) (3.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.8/site-packages (from requests<3,>=2.0.0->wandb) (2021.5.30)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /opt/conda/lib/python3.8/site-packages (from requests<3,>=2.0.0->wandb) (2.0.0)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.8/site-packages (from requests<3,>=2.0.0->wandb) (1.26.7)\n",
      "Collecting termcolor<2.0.0,>=1.1.0\n",
      "  Downloading termcolor-1.1.0.tar.gz (3.9 kB)\n",
      "Building wheels for collected packages: promise, termcolor, pathtools\n",
      "  Building wheel for promise (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for promise: filename=promise-2.3-py3-none-any.whl size=21502 sha256=e6336627ce9abbbae580b14e061b6441362c7c527458ef39a318050d8b17b9bd\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-hd6u987w/wheels/54/aa/01/724885182f93150035a2a91bce34a12877e8067a97baaf5dc8\n",
      "  Building wheel for termcolor (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for termcolor: filename=termcolor-1.1.0-py3-none-any.whl size=4847 sha256=0c3aeac442cf2aedd196b90ab9153636eb84221c0fcd6402e42fe51a6a77d50d\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-hd6u987w/wheels/a0/16/9c/5473df82468f958445479c59e784896fa24f4a5fc024b0f501\n",
      "  Building wheel for pathtools (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for pathtools: filename=pathtools-0.1.2-py3-none-any.whl size=8807 sha256=d0471014e226a70776d525077646325b5ab0b83091992049eb2b0bbe283d4090\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-hd6u987w/wheels/4c/8e/7e/72fbc243e1aeecae64a96875432e70d4e92f3d2d18123be004\n",
      "Successfully built promise termcolor pathtools\n",
      "Installing collected packages: smmap, termcolor, gitdb, yaspin, shortuuid, sentry-sdk, protobuf, promise, pathtools, GitPython, docker-pycreds, wandb\n",
      "  Attempting uninstall: protobuf\n",
      "    Found existing installation: protobuf 3.11.2\n",
      "    Uninstalling protobuf-3.11.2:\n",
      "      Successfully uninstalled protobuf-3.11.2\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "matrixprofile 1.1.10 requires protobuf==3.11.2, but you have protobuf 3.19.4 which is incompatible.\u001b[0m\n",
      "Successfully installed GitPython-3.1.27 docker-pycreds-0.4.0 gitdb-4.0.9 pathtools-0.1.2 promise-2.3 protobuf-3.19.4 sentry-sdk-1.5.5 shortuuid-1.0.8 smmap-5.0.0 termcolor-1.1.0 wandb-0.12.10 yaspin-2.1.0\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n",
      "Looking in indexes: https://pypi.org/simple, https://pypi.ngc.nvidia.com\n",
      "Collecting optuna\n",
      "  Downloading optuna-2.10.0-py3-none-any.whl (308 kB)\n",
      "\u001b[K     |████████████████████████████████| 308 kB 6.7 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting cliff\n",
      "  Downloading cliff-3.10.1-py3-none-any.whl (81 kB)\n",
      "\u001b[K     |████████████████████████████████| 81 kB 32.2 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.8/site-packages (from optuna) (21.0)\n",
      "Collecting cmaes>=0.8.2\n",
      "  Downloading cmaes-0.8.2-py3-none-any.whl (15 kB)\n",
      "Requirement already satisfied: scipy!=1.4.0 in /opt/conda/lib/python3.8/site-packages (from optuna) (1.6.3)\n",
      "Collecting sqlalchemy>=1.1.0\n",
      "  Downloading SQLAlchemy-1.4.31-cp38-cp38-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.6 MB)\n",
      "\u001b[K     |████████████████████████████████| 1.6 MB 31.6 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: PyYAML in /opt/conda/lib/python3.8/site-packages (from optuna) (5.4.1)\n",
      "Collecting colorlog\n",
      "  Downloading colorlog-6.6.0-py2.py3-none-any.whl (11 kB)\n",
      "Requirement already satisfied: numpy in /opt/conda/lib/python3.8/site-packages (from optuna) (1.20.0)\n",
      "Collecting alembic\n",
      "  Downloading alembic-1.7.6-py3-none-any.whl (210 kB)\n",
      "\u001b[K     |████████████████████████████████| 210 kB 29.1 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: tqdm in /opt/conda/lib/python3.8/site-packages (from optuna) (4.62.3)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in /opt/conda/lib/python3.8/site-packages (from packaging>=20.0->optuna) (2.4.7)\n",
      "Collecting greenlet!=0.4.17\n",
      "  Downloading greenlet-1.1.2-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (156 kB)\n",
      "\u001b[K     |████████████████████████████████| 156 kB 29.0 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting importlib-resources\n",
      "  Downloading importlib_resources-5.4.0-py3-none-any.whl (28 kB)\n",
      "Collecting importlib-metadata\n",
      "  Downloading importlib_metadata-4.11.1-py3-none-any.whl (17 kB)\n",
      "Collecting Mako\n",
      "  Downloading Mako-1.1.6-py2.py3-none-any.whl (75 kB)\n",
      "\u001b[K     |████████████████████████████████| 75 kB 33.6 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting pbr!=2.1.0,>=2.0.0\n",
      "  Downloading pbr-5.8.1-py2.py3-none-any.whl (113 kB)\n",
      "\u001b[K     |████████████████████████████████| 113 kB 30.3 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting autopage>=0.4.0\n",
      "  Downloading autopage-0.5.0-py3-none-any.whl (29 kB)\n",
      "Requirement already satisfied: PrettyTable>=0.7.2 in /opt/conda/lib/python3.8/site-packages (from cliff->optuna) (2.2.1)\n",
      "Collecting stevedore>=2.0.1\n",
      "  Downloading stevedore-3.5.0-py3-none-any.whl (49 kB)\n",
      "\u001b[K     |████████████████████████████████| 49 kB 46.7 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting cmd2>=1.0.0\n",
      "  Downloading cmd2-2.3.3-py3-none-any.whl (149 kB)\n",
      "\u001b[K     |████████████████████████████████| 149 kB 28.9 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting pyperclip>=1.6\n",
      "  Downloading pyperclip-1.8.2.tar.gz (20 kB)\n",
      "Requirement already satisfied: attrs>=16.3.0 in /opt/conda/lib/python3.8/site-packages (from cmd2>=1.0.0->cliff->optuna) (21.2.0)\n",
      "Requirement already satisfied: wcwidth>=0.1.7 in /opt/conda/lib/python3.8/site-packages (from cmd2>=1.0.0->cliff->optuna) (0.2.5)\n",
      "Collecting zipp>=0.5\n",
      "  Downloading zipp-3.7.0-py3-none-any.whl (5.3 kB)\n",
      "Requirement already satisfied: MarkupSafe>=0.9.2 in /opt/conda/lib/python3.8/site-packages (from Mako->alembic->optuna) (2.0.1)\n",
      "Building wheels for collected packages: pyperclip\n",
      "  Building wheel for pyperclip (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for pyperclip: filename=pyperclip-1.8.2-py3-none-any.whl size=11136 sha256=12def99d5650f5d4e0104ba3a20eeed539e129dc6e3f334116f189f6a3fc9a22\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-1nuelfvw/wheels/7f/1a/65/84ff8c386bec21fca6d220ea1f5498a0367883a78dd5ba6122\n",
      "Successfully built pyperclip\n",
      "Installing collected packages: zipp, pyperclip, pbr, greenlet, stevedore, sqlalchemy, Mako, importlib-resources, importlib-metadata, cmd2, autopage, colorlog, cmaes, cliff, alembic, optuna\n",
      "Successfully installed Mako-1.1.6 alembic-1.7.6 autopage-0.5.0 cliff-3.10.1 cmaes-0.8.2 cmd2-2.3.3 colorlog-6.6.0 greenlet-1.1.2 importlib-metadata-4.11.1 importlib-resources-5.4.0 optuna-2.10.0 pbr-5.8.1 pyperclip-1.8.2 sqlalchemy-1.4.31 stevedore-3.5.0 zipp-3.7.0\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install llvmlite==0.38.0 --ignore-installed\n",
    "!pip install tsai -U >> /dev/null\n",
    "!pip install --upgrade sktime==0.9.0\n",
    "!pip3 install --upgrade numpy==1.20\n",
    "!pip install wandb -U\n",
    "!pip install optuna -U"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "876d5410",
   "metadata": {
    "gradient": {
     "editing": false,
     "execution_count": 1,
     "id": "f4a680db-74ef-4660-877c-83ecb4895ef1",
     "kernelId": "e731389a-9397-4fee-9c6b-efa8de72a05c"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "OMP: Info #271: omp_set_nested routine deprecated, please use omp_set_max_active_levels instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "os             : Linux-5.4.0-89-generic-x86_64-with-glibc2.10\n",
      "python         : 3.8.12\n",
      "tsai           : 0.2.25\n",
      "fastai         : 2.5.3\n",
      "fastcore       : 1.3.27\n",
      "wandb          : 0.12.10\n",
      "torch          : 1.10.0a0+0aef44c\n",
      "n_cpus         : 8\n",
      "device         : cuda (Quadro RTX 4000)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from tsai.all import *\n",
    "from scipy import io\n",
    "import wandb\n",
    "from fastai.callback.wandb import *\n",
    "from getdata import get_fofu_data\n",
    "my_setup(wandb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a08ca14b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wandb.login()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5cb16e9",
   "metadata": {},
   "source": [
    "Define the sweep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d1f177b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Create sweep with ID: 8p3dse0m\n",
      "Sweep URL: https://wandb.ai/sashalikesplanes/HOClass/sweeps/8p3dse0m\n"
     ]
    }
   ],
   "source": [
    "sweep_config = {\n",
    "    \"name\" : \"Sweep on LR, Wd, and Dropout using Standardized data with accuracy as metric\",\n",
    "    \"project\" : \"HOClass\",\n",
    "    \"description\" : \"A sweep using Bayes search, adjusted range on Wd, decreased lower bound on Conv_dropout factor, added data standardization using Verskeer method, changed bottleneck to always off, switched residual to always off, switched LR to use a range\",\n",
    "    \"method\" : \"bayes\",\n",
    "    \"metric\": {\n",
    "        \"name\": \"accuracy\",\n",
    "        \"goal\": \"maximize\",\n",
    "    },\n",
    "    \"early_terminate\" : {\n",
    "        \"type\" : \"hyperband\",\n",
    "        \"min_iter\" : 7500,\n",
    "        \"eta\" : 2,\n",
    "    },\n",
    "  \n",
    "    \"parameters\" : {\n",
    "        \"epochs\" : {\n",
    "            \"value\" : 50\n",
    "        },\n",
    "        \"lr\" : {\n",
    "            \"min\": 0.0025,\n",
    "            \"max\" : 0.01,\n",
    "        },\n",
    "        \"batch_size\" : {\n",
    "            \"value\" : 64\n",
    "        },\n",
    "        \"nf\" : {\n",
    "            \"value\" : 24,\n",
    "        },\n",
    "        \"ks\" : {\n",
    "            \"value\" : 64\n",
    "        },\n",
    "        \"bottleneck\" : {\n",
    "            \"value\" : False\n",
    "        },\n",
    "        \"bottleneck_size\"  : {\n",
    "            \"value\" : 0\n",
    "        },\n",
    "        \"depth\"  : {\n",
    "            \"value\" :  6\n",
    "        },\n",
    "        \"residual\"  : {\n",
    "            \"value\" : False\n",
    "        },\n",
    "        \"valid_pct\"  : {\n",
    "            \"value\" : 0.2\n",
    "        },\n",
    "        \"wd\" : {\n",
    "            \"distribution\" : \"log_uniform\",\n",
    "            \"min\" : -2,\n",
    "            \"max\" : -1,\n",
    "        },\n",
    "        \"conv_dropout\" : {\n",
    "            \"min\" : 0.05,\n",
    "            \"max\" : 0.3,\n",
    "        },\n",
    "        \"variables\" : {\n",
    "            \"value\" : ['e', 'u', 'x', 'dedt', 'dudt', 'dxdt']\n",
    "        },\n",
    "        \"bn\" : {\n",
    "            \"values\" : [True, False]\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "sweep_id = wandb.sweep(sweep_config, project=\"HOClass\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9aead10f",
   "metadata": {},
   "source": [
    "Current sweep id : r6d4njmc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6588fd83",
   "metadata": {},
   "outputs": [],
   "source": [
    "sweep_id = \"8p3dse0m\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "2cdf04ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train():\n",
    "    with wandb.init(project=\"HOClass\") as run:\n",
    "        config = wandb.config\n",
    "        # Get the data sets\n",
    "        print(config)\n",
    "        X, Y, splits = get_fofu_data(valid_pct=config[\"valid_pct\"])\n",
    "        print(Y)\n",
    "        tfms = [None, Categorize()]\n",
    "        dsets = TSDatasets(X, Y, tfms=tfms, splits=splits, inplace=True)\n",
    "        \n",
    "        # Get data loaders\n",
    "        batch_tfms = None #[TSStandardize(by_var=config[\"standardize_by_var\"], by_sample=config[\"standardize_by_sample\"])]\n",
    "        dls = TSDataLoaders.from_dsets(dsets.train, dsets.valid, bs=config[\"batch_size\"], batch_tfms=batch_tfms, shuffle=True, num_workers=8)\n",
    "        \n",
    "        # Get model\n",
    "        cbs = [SaveModelCallback(), WandbCallback(log_preds=True, log_model=False, dataset_name='valid runs random, standardized')]\n",
    "        model = InceptionTimePlus(dls.vars, dls.c, nf=config[\"nf\"], bn=config[\"bn\"], ks=config[\"ks\"], bottleneck=config[\"bottleneck\"],\n",
    "                                  conv_dropout=config[\"conv_dropout\"], depth=config[\"depth\"])\n",
    "        # Get learner\n",
    "        learner = Learner(dls, model, opt_func=Adam, metrics=accuracy, wd=config[\"wd\"], cbs=cbs)\n",
    "        # Execute fit one cycle training\n",
    "        learner.fit_one_cycle(config[\"epochs\"], lr_max=config[\"lr\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65bf0a68",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: e5tb607k with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 64\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbn: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbottleneck: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbottleneck_size: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tconv_dropout: 0.2824299867984535\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdepth: 6\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepochs: 50\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tks: 64\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlr: 0.008846803181183608\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnf: 24\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tresidual: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tvalid_pct: 0.2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tvariables: ['e', 'u', 'x', 'dedt', 'dudt', 'dxdt']\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \twd: 0.209723833601737\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Ignored wandb.init() arg project when running a sweep.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                    Syncing run <strong><a href=\"https://wandb.ai/sashalikesplanes/HOClass/runs/e5tb607k\" target=\"_blank\">wild-sweep-1</a></strong> to <a href=\"https://wandb.ai/sashalikesplanes/HOClass\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">docs</a>).<br/>\n",
       "Sweep page: <a href=\"https://wandb.ai/sashalikesplanes/HOClass/sweeps/8p3dse0m\" target=\"_blank\">https://wandb.ai/sashalikesplanes/HOClass/sweeps/8p3dse0m</a><br/>\n",
       "\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'batch_size': 64, 'bn': False, 'bottleneck': False, 'bottleneck_size': 0, 'conv_dropout': 0.2824299867984535, 'depth': 6, 'epochs': 50, 'ks': 64, 'lr': 0.008846803181183608, 'nf': 24, 'residual': False, 'valid_pct': 0.2, 'variables': ['e', 'u', 'x', 'dedt', 'dudt', 'dxdt'], 'wd': 0.209723833601737}\n",
      "[0. 0. 0. ... 2. 2. 2.]\n",
      "WandbCallback was not able to prepare a DataLoader for logging prediction samples -> Expected an input of type in \n",
      "  - <class 'numpy.float64'>\n",
      "  - <class 'fastai.torch_core.TensorCategory'>\n",
      " but got <class 'numpy.ndarray'>\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.522070</td>\n",
       "      <td>0.625497</td>\n",
       "      <td>0.716186</td>\n",
       "      <td>00:30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.431552</td>\n",
       "      <td>0.636959</td>\n",
       "      <td>0.722838</td>\n",
       "      <td>00:30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.388673</td>\n",
       "      <td>0.643891</td>\n",
       "      <td>0.724818</td>\n",
       "      <td>00:29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.395759</td>\n",
       "      <td>0.643972</td>\n",
       "      <td>0.724660</td>\n",
       "      <td>00:30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.369088</td>\n",
       "      <td>1.030055</td>\n",
       "      <td>0.645629</td>\n",
       "      <td>00:30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.337651</td>\n",
       "      <td>0.645701</td>\n",
       "      <td>0.741606</td>\n",
       "      <td>00:29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.343398</td>\n",
       "      <td>0.901024</td>\n",
       "      <td>0.678809</td>\n",
       "      <td>00:29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.344625</td>\n",
       "      <td>1.587003</td>\n",
       "      <td>0.609598</td>\n",
       "      <td>00:30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.372529</td>\n",
       "      <td>1.601912</td>\n",
       "      <td>0.627732</td>\n",
       "      <td>00:30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.359503</td>\n",
       "      <td>1.487487</td>\n",
       "      <td>0.624169</td>\n",
       "      <td>00:30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.367268</td>\n",
       "      <td>0.880874</td>\n",
       "      <td>0.692746</td>\n",
       "      <td>00:30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>0.356474</td>\n",
       "      <td>1.495915</td>\n",
       "      <td>0.631691</td>\n",
       "      <td>00:30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>0.368658</td>\n",
       "      <td>1.439708</td>\n",
       "      <td>0.642699</td>\n",
       "      <td>00:30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>0.336827</td>\n",
       "      <td>1.386460</td>\n",
       "      <td>0.638898</td>\n",
       "      <td>00:30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>0.340065</td>\n",
       "      <td>1.362862</td>\n",
       "      <td>0.648480</td>\n",
       "      <td>00:30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>0.346839</td>\n",
       "      <td>1.738326</td>\n",
       "      <td>0.646421</td>\n",
       "      <td>00:29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>0.373856</td>\n",
       "      <td>2.195827</td>\n",
       "      <td>0.624564</td>\n",
       "      <td>00:29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>0.342338</td>\n",
       "      <td>0.856979</td>\n",
       "      <td>0.675087</td>\n",
       "      <td>00:31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>0.343998</td>\n",
       "      <td>1.642771</td>\n",
       "      <td>0.641590</td>\n",
       "      <td>00:30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>0.318611</td>\n",
       "      <td>1.763370</td>\n",
       "      <td>0.617992</td>\n",
       "      <td>00:30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>0.327302</td>\n",
       "      <td>1.982205</td>\n",
       "      <td>0.637472</td>\n",
       "      <td>00:31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21</td>\n",
       "      <td>0.341955</td>\n",
       "      <td>2.854795</td>\n",
       "      <td>0.510136</td>\n",
       "      <td>00:30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22</td>\n",
       "      <td>0.327001</td>\n",
       "      <td>1.904433</td>\n",
       "      <td>0.632008</td>\n",
       "      <td>00:31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23</td>\n",
       "      <td>0.311505</td>\n",
       "      <td>1.775674</td>\n",
       "      <td>0.628366</td>\n",
       "      <td>00:31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24</td>\n",
       "      <td>0.312451</td>\n",
       "      <td>2.040842</td>\n",
       "      <td>0.600729</td>\n",
       "      <td>00:30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25</td>\n",
       "      <td>0.298973</td>\n",
       "      <td>2.583938</td>\n",
       "      <td>0.600887</td>\n",
       "      <td>00:30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26</td>\n",
       "      <td>0.298307</td>\n",
       "      <td>2.399648</td>\n",
       "      <td>0.626069</td>\n",
       "      <td>00:30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27</td>\n",
       "      <td>0.280312</td>\n",
       "      <td>2.461185</td>\n",
       "      <td>0.551394</td>\n",
       "      <td>00:30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28</td>\n",
       "      <td>0.279416</td>\n",
       "      <td>2.210656</td>\n",
       "      <td>0.589088</td>\n",
       "      <td>00:30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29</td>\n",
       "      <td>0.283193</td>\n",
       "      <td>2.779196</td>\n",
       "      <td>0.556858</td>\n",
       "      <td>00:30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>0.273192</td>\n",
       "      <td>2.179315</td>\n",
       "      <td>0.583149</td>\n",
       "      <td>00:30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>31</td>\n",
       "      <td>0.258562</td>\n",
       "      <td>1.735724</td>\n",
       "      <td>0.633354</td>\n",
       "      <td>00:31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>32</td>\n",
       "      <td>0.240267</td>\n",
       "      <td>1.931542</td>\n",
       "      <td>0.628999</td>\n",
       "      <td>00:30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>33</td>\n",
       "      <td>0.242122</td>\n",
       "      <td>2.938321</td>\n",
       "      <td>0.634542</td>\n",
       "      <td>00:31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>34</td>\n",
       "      <td>0.226100</td>\n",
       "      <td>3.199878</td>\n",
       "      <td>0.589246</td>\n",
       "      <td>00:30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>35</td>\n",
       "      <td>0.232086</td>\n",
       "      <td>2.502011</td>\n",
       "      <td>0.586633</td>\n",
       "      <td>00:30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>36</td>\n",
       "      <td>0.203722</td>\n",
       "      <td>3.011521</td>\n",
       "      <td>0.626623</td>\n",
       "      <td>00:31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>37</td>\n",
       "      <td>0.200440</td>\n",
       "      <td>3.202492</td>\n",
       "      <td>0.617279</td>\n",
       "      <td>00:30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>38</td>\n",
       "      <td>0.192265</td>\n",
       "      <td>3.278418</td>\n",
       "      <td>0.631137</td>\n",
       "      <td>00:31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>39</td>\n",
       "      <td>0.177128</td>\n",
       "      <td>2.972801</td>\n",
       "      <td>0.612765</td>\n",
       "      <td>00:30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>0.152082</td>\n",
       "      <td>3.362748</td>\n",
       "      <td>0.583624</td>\n",
       "      <td>00:31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>41</td>\n",
       "      <td>0.146208</td>\n",
       "      <td>3.382126</td>\n",
       "      <td>0.628049</td>\n",
       "      <td>00:32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>42</td>\n",
       "      <td>0.129113</td>\n",
       "      <td>4.092802</td>\n",
       "      <td>0.588533</td>\n",
       "      <td>00:30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>43</td>\n",
       "      <td>0.111223</td>\n",
       "      <td>4.010357</td>\n",
       "      <td>0.583149</td>\n",
       "      <td>00:31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>44</td>\n",
       "      <td>0.110820</td>\n",
       "      <td>3.998357</td>\n",
       "      <td>0.599303</td>\n",
       "      <td>00:31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>45</td>\n",
       "      <td>0.094808</td>\n",
       "      <td>4.278877</td>\n",
       "      <td>0.585366</td>\n",
       "      <td>00:31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>46</td>\n",
       "      <td>0.078627</td>\n",
       "      <td>4.375214</td>\n",
       "      <td>0.599937</td>\n",
       "      <td>00:31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>47</td>\n",
       "      <td>0.071383</td>\n",
       "      <td>4.355155</td>\n",
       "      <td>0.609281</td>\n",
       "      <td>00:31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>48</td>\n",
       "      <td>0.066322</td>\n",
       "      <td>4.295187</td>\n",
       "      <td>0.609281</td>\n",
       "      <td>00:30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>49</td>\n",
       "      <td>0.065604</td>\n",
       "      <td>4.338734</td>\n",
       "      <td>0.600253</td>\n",
       "      <td>00:31</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Better model found at epoch 0 with valid_loss value: 0.625497043132782.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<br/>Waiting for W&B process to finish, PID 8423... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "wandb.agent(sweep_id, project=\"HOClass\", function=train, count=25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "562765bd",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "                    Syncing run <strong><a href=\"https://wandb.ai/sashalikesplanes/HOClass/runs/2y7jk59p\" target=\"_blank\">All vars, dropout=0.1</a></strong> to <a href=\"https://wandb.ai/sashalikesplanes/HOClass\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">docs</a>).<br/>\n",
       "\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WandbCallback was not able to prepare a DataLoader for logging prediction samples -> Expected an input of type in \n",
      "  - <class 'numpy.float64'>\n",
      "  - <class 'fastai.torch_core.TensorCategory'>\n",
      " but got <class 'numpy.ndarray'>\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.693357</td>\n",
       "      <td>0.679048</td>\n",
       "      <td>0.689326</td>\n",
       "      <td>00:33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.643246</td>\n",
       "      <td>0.697719</td>\n",
       "      <td>0.680731</td>\n",
       "      <td>00:30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.632047</td>\n",
       "      <td>0.630845</td>\n",
       "      <td>0.714565</td>\n",
       "      <td>00:31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.605246</td>\n",
       "      <td>0.615372</td>\n",
       "      <td>0.721675</td>\n",
       "      <td>00:31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.583077</td>\n",
       "      <td>0.607768</td>\n",
       "      <td>0.724879</td>\n",
       "      <td>00:31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.574802</td>\n",
       "      <td>0.621701</td>\n",
       "      <td>0.725270</td>\n",
       "      <td>00:31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.561086</td>\n",
       "      <td>0.559472</td>\n",
       "      <td>0.748398</td>\n",
       "      <td>00:31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.546220</td>\n",
       "      <td>0.546934</td>\n",
       "      <td>0.758400</td>\n",
       "      <td>00:31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.527859</td>\n",
       "      <td>0.567852</td>\n",
       "      <td>0.748789</td>\n",
       "      <td>00:32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.518102</td>\n",
       "      <td>0.535232</td>\n",
       "      <td>0.766604</td>\n",
       "      <td>00:32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.489228</td>\n",
       "      <td>0.516300</td>\n",
       "      <td>0.771839</td>\n",
       "      <td>00:32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>0.464804</td>\n",
       "      <td>0.527555</td>\n",
       "      <td>0.770980</td>\n",
       "      <td>00:31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>0.458069</td>\n",
       "      <td>0.481247</td>\n",
       "      <td>0.792233</td>\n",
       "      <td>00:31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>0.435025</td>\n",
       "      <td>0.527378</td>\n",
       "      <td>0.773090</td>\n",
       "      <td>00:31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>0.428287</td>\n",
       "      <td>0.484599</td>\n",
       "      <td>0.792233</td>\n",
       "      <td>00:33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>0.411029</td>\n",
       "      <td>0.477255</td>\n",
       "      <td>0.794030</td>\n",
       "      <td>00:33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>0.398744</td>\n",
       "      <td>0.474553</td>\n",
       "      <td>0.797390</td>\n",
       "      <td>00:32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>0.372345</td>\n",
       "      <td>0.498747</td>\n",
       "      <td>0.790123</td>\n",
       "      <td>00:32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>0.369769</td>\n",
       "      <td>0.486055</td>\n",
       "      <td>0.800985</td>\n",
       "      <td>00:31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>0.333214</td>\n",
       "      <td>0.467233</td>\n",
       "      <td>0.804735</td>\n",
       "      <td>00:32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>0.357529</td>\n",
       "      <td>0.482620</td>\n",
       "      <td>0.795749</td>\n",
       "      <td>00:32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21</td>\n",
       "      <td>0.318060</td>\n",
       "      <td>0.471996</td>\n",
       "      <td>0.806767</td>\n",
       "      <td>00:32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22</td>\n",
       "      <td>0.301957</td>\n",
       "      <td>0.511975</td>\n",
       "      <td>0.795046</td>\n",
       "      <td>00:32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23</td>\n",
       "      <td>0.279500</td>\n",
       "      <td>0.504461</td>\n",
       "      <td>0.800438</td>\n",
       "      <td>00:33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24</td>\n",
       "      <td>0.271586</td>\n",
       "      <td>0.551572</td>\n",
       "      <td>0.790123</td>\n",
       "      <td>00:32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25</td>\n",
       "      <td>0.255777</td>\n",
       "      <td>0.552581</td>\n",
       "      <td>0.801844</td>\n",
       "      <td>00:31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26</td>\n",
       "      <td>0.223307</td>\n",
       "      <td>0.570906</td>\n",
       "      <td>0.790436</td>\n",
       "      <td>00:31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27</td>\n",
       "      <td>0.202932</td>\n",
       "      <td>0.595789</td>\n",
       "      <td>0.796531</td>\n",
       "      <td>00:33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28</td>\n",
       "      <td>0.203084</td>\n",
       "      <td>0.593943</td>\n",
       "      <td>0.789030</td>\n",
       "      <td>00:31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29</td>\n",
       "      <td>0.185731</td>\n",
       "      <td>0.596985</td>\n",
       "      <td>0.792780</td>\n",
       "      <td>00:33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>0.158634</td>\n",
       "      <td>0.601890</td>\n",
       "      <td>0.795359</td>\n",
       "      <td>00:31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>31</td>\n",
       "      <td>0.161413</td>\n",
       "      <td>0.626309</td>\n",
       "      <td>0.790905</td>\n",
       "      <td>00:32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>32</td>\n",
       "      <td>0.143742</td>\n",
       "      <td>0.629889</td>\n",
       "      <td>0.799656</td>\n",
       "      <td>00:31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>33</td>\n",
       "      <td>0.118370</td>\n",
       "      <td>0.620835</td>\n",
       "      <td>0.802860</td>\n",
       "      <td>00:31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>34</td>\n",
       "      <td>0.104415</td>\n",
       "      <td>0.690287</td>\n",
       "      <td>0.799344</td>\n",
       "      <td>00:33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>35</td>\n",
       "      <td>0.095774</td>\n",
       "      <td>0.719892</td>\n",
       "      <td>0.791608</td>\n",
       "      <td>00:32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>36</td>\n",
       "      <td>0.086755</td>\n",
       "      <td>0.728997</td>\n",
       "      <td>0.796140</td>\n",
       "      <td>00:32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>37</td>\n",
       "      <td>0.077018</td>\n",
       "      <td>0.753659</td>\n",
       "      <td>0.794265</td>\n",
       "      <td>00:33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>38</td>\n",
       "      <td>0.072345</td>\n",
       "      <td>0.778262</td>\n",
       "      <td>0.793952</td>\n",
       "      <td>00:34</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>39</td>\n",
       "      <td>0.065246</td>\n",
       "      <td>0.765150</td>\n",
       "      <td>0.797000</td>\n",
       "      <td>00:31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>0.061837</td>\n",
       "      <td>0.797460</td>\n",
       "      <td>0.793796</td>\n",
       "      <td>00:32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>41</td>\n",
       "      <td>0.054196</td>\n",
       "      <td>0.796638</td>\n",
       "      <td>0.796687</td>\n",
       "      <td>00:32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>42</td>\n",
       "      <td>0.038567</td>\n",
       "      <td>0.811775</td>\n",
       "      <td>0.795906</td>\n",
       "      <td>00:32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>43</td>\n",
       "      <td>0.042165</td>\n",
       "      <td>0.841409</td>\n",
       "      <td>0.795827</td>\n",
       "      <td>00:33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>44</td>\n",
       "      <td>0.042326</td>\n",
       "      <td>0.832049</td>\n",
       "      <td>0.797156</td>\n",
       "      <td>00:32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>45</td>\n",
       "      <td>0.045586</td>\n",
       "      <td>0.867643</td>\n",
       "      <td>0.791217</td>\n",
       "      <td>00:33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>46</td>\n",
       "      <td>0.035273</td>\n",
       "      <td>0.838097</td>\n",
       "      <td>0.795437</td>\n",
       "      <td>00:32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>47</td>\n",
       "      <td>0.040044</td>\n",
       "      <td>0.845941</td>\n",
       "      <td>0.796062</td>\n",
       "      <td>00:32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>48</td>\n",
       "      <td>0.031059</td>\n",
       "      <td>0.822033</td>\n",
       "      <td>0.796843</td>\n",
       "      <td>00:31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>49</td>\n",
       "      <td>0.036295</td>\n",
       "      <td>0.858631</td>\n",
       "      <td>0.793952</td>\n",
       "      <td>00:33</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Better model found at epoch 0 with valid_loss value: 0.6790475249290466.\n",
      "Better model found at epoch 2 with valid_loss value: 0.6308445334434509.\n",
      "Better model found at epoch 3 with valid_loss value: 0.6153716444969177.\n",
      "Better model found at epoch 4 with valid_loss value: 0.6077684760093689.\n",
      "Better model found at epoch 6 with valid_loss value: 0.5594715476036072.\n",
      "Better model found at epoch 7 with valid_loss value: 0.5469342470169067.\n",
      "Better model found at epoch 9 with valid_loss value: 0.5352318286895752.\n",
      "Better model found at epoch 10 with valid_loss value: 0.5163002014160156.\n",
      "Better model found at epoch 12 with valid_loss value: 0.4812472462654114.\n",
      "Better model found at epoch 15 with valid_loss value: 0.47725504636764526.\n",
      "Better model found at epoch 16 with valid_loss value: 0.4745529592037201.\n",
      "Better model found at epoch 19 with valid_loss value: 0.467233270406723.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<br/>Waiting for W&B process to finish, PID 561... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\">\n",
       "<h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>accuracy</td><td>▁▁▃▃▃▅▅▅▆▆▇▆▇▇▇█▇█▇██▇▇▇▇▇██▇▇▇▇▇▇▇▇▇▇▇▇</td></tr><tr><td>epoch</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>eps_0</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>lr_0</td><td>▁▁▂▃▄▅▆▇▇██████▇▇▇▇▆▆▆▅▅▅▄▄▄▃▃▃▂▂▂▁▁▁▁▁▁</td></tr><tr><td>mom_0</td><td>██▇▆▅▄▃▂▂▁▁▁▁▁▁▂▂▂▂▃▃▃▄▄▄▅▅▅▆▆▆▇▇▇██████</td></tr><tr><td>raw_loss</td><td>█▇▆▇▅▆▇▇▅▇▅▅▅▄▄▄▄▃▃▄▃▄▄▃▃▂▂▂▁▂▁▂▁▂▁▁▁▁▁▁</td></tr><tr><td>sqr_mom_0</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_loss</td><td>█▇▇▇▆▆▆▆▆▅▅▅▅▄▄▄▄▄▃▃▃▃▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>valid_loss</td><td>▅▅▄▄▄▃▂▃▂▂▁▂▁▁▁▁▁▁▂▂▂▃▃▃▃▄▄▄▅▆▆▆▇▇▇██▇██</td></tr><tr><td>wd_0</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\">\n",
       "<h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>accuracy</td><td>0.79395</td></tr><tr><td>epoch</td><td>50</td></tr><tr><td>eps_0</td><td>1e-05</td></tr><tr><td>lr_0</td><td>0.0</td></tr><tr><td>mom_0</td><td>0.95</td></tr><tr><td>raw_loss</td><td>0.0127</td></tr><tr><td>sqr_mom_0</td><td>0.99</td></tr><tr><td>train_loss</td><td>0.0363</td></tr><tr><td>valid_loss</td><td>0.85863</td></tr><tr><td>wd_0</td><td>1e-05</td></tr></table>\n",
       "</div></div>\n",
       "Synced 6 W&B file(s), 0 media file(s), 1 artifact file(s) and 1 other file(s)\n",
       "<br/>Synced <strong style=\"color:#cdcd00\">All vars, dropout=0.1</strong>: <a href=\"https://wandb.ai/sashalikesplanes/HOClass/runs/2y7jk59p\" target=\"_blank\">https://wandb.ai/sashalikesplanes/HOClass/runs/2y7jk59p</a><br/>\n",
       "Find logs at: <code>./wandb/run-20220201_114745-2y7jk59p/logs</code><br/>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "config = AttrDict (\n",
    "    batch_size = 64,\n",
    "    nf = 24, # number of convolutional filters\n",
    "    ks = 64, # largest kernel size of convolution filter\n",
    "    bottleneck = False, # use bottleneck\n",
    "    bottleneck_size = 0,\n",
    "    depth = 6, # number of inception models, best 3\n",
    "    residual = True, # best False\n",
    "    opt_func = Adam,\n",
    "    valid_pct = 0.2,\n",
    "    wd = 5e-6, # weight decay\n",
    "    lr = 1e-3,\n",
    "    n_epochs = 50,\n",
    "    variables = ['e', 'u', 'x' 'dedt', 'dudt', 'dxdt'],\n",
    "    conv_dropout = 0.1,\n",
    "    standardize_by_var = True,\n",
    "    standardize_by_sample = True,\n",
    "    )\n",
    "\n",
    "\n",
    "with wandb.init(project=\"HOClass\", config=config, group=\"InceptionTimePlus\", name=\"All vars, dropout=0.1\", save_code=True,):\n",
    "\n",
    "    \n",
    "   "
   ]
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "conda-root-py",
   "name": "pytorch-gpu.1-10.m86",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/pytorch-gpu.1-10:m86"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
