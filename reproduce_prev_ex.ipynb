{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f1cd7ce9",
   "metadata": {
    "gradient": {
     "editing": false,
     "id": "5c065e38-f3ab-4302-b63e-b1eb7bd47673",
     "kernelId": "e731389a-9397-4fee-9c6b-efa8de72a05c"
    }
   },
   "source": [
    "### Reproduction of Inception Time algo applied to Fofu dataset from paper by Verket"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d81deb6c",
   "metadata": {},
   "source": [
    "Run cells to set github automatically"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5435141c",
   "metadata": {},
   "outputs": [],
   "source": [
    "!git config --global user.email \"a.kiselev@student.tudelft.nl\"\n",
    "!git config --global user.name \"Alexander Kiselev\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59f4e099",
   "metadata": {},
   "source": [
    "Calls to install on the needed packages as the environment gets wiped every time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d14ab5c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://pypi.ngc.nvidia.com\n",
      "Collecting llvmlite==0.38.0\n",
      "  Downloading llvmlite-0.38.0-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (34.5 MB)\n",
      "\u001b[K     |████████████████████████████████| 34.5 MB 9.7 MB/s eta 0:00:011\n",
      "\u001b[?25hInstalling collected packages: llvmlite\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "numba 0.52.0 requires llvmlite<0.36,>=0.35.0, but you have llvmlite 0.38.0 which is incompatible.\u001b[0m\n",
      "Successfully installed llvmlite-0.38.0\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n",
      "Looking in indexes: https://pypi.org/simple, https://pypi.ngc.nvidia.com\n",
      "Requirement already satisfied: sktime==0.9.0 in /opt/conda/lib/python3.8/site-packages (0.9.0)\n",
      "Requirement already satisfied: numpy<=1.19.3 in /opt/conda/lib/python3.8/site-packages (from sktime==0.9.0) (1.19.3)\n",
      "Requirement already satisfied: statsmodels>=0.12.1 in /opt/conda/lib/python3.8/site-packages (from sktime==0.9.0) (0.12.1)\n",
      "Requirement already satisfied: deprecated>=1.2.13 in /opt/conda/lib/python3.8/site-packages (from sktime==0.9.0) (1.2.13)\n",
      "Requirement already satisfied: pandas>=1.1.0 in /opt/conda/lib/python3.8/site-packages (from sktime==0.9.0) (1.4.1)\n",
      "Requirement already satisfied: scikit-learn>=0.24.0 in /opt/conda/lib/python3.8/site-packages (from sktime==0.9.0) (1.0.2)\n",
      "Requirement already satisfied: numba>=0.53 in /opt/conda/lib/python3.8/site-packages (from sktime==0.9.0) (0.54.1)\n",
      "Requirement already satisfied: wheel in /opt/conda/lib/python3.8/site-packages (from sktime==0.9.0) (0.37.0)\n",
      "Requirement already satisfied: wrapt<2,>=1.10 in /opt/conda/lib/python3.8/site-packages (from deprecated>=1.2.13->sktime==0.9.0) (1.13.3)\n",
      "Requirement already satisfied: setuptools in /opt/conda/lib/python3.8/site-packages (from numba>=0.53->sktime==0.9.0) (58.2.0)\n",
      "Requirement already satisfied: llvmlite<0.38,>=0.37.0rc1 in /opt/conda/lib/python3.8/site-packages (from numba>=0.53->sktime==0.9.0) (0.37.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in /opt/conda/lib/python3.8/site-packages (from pandas>=1.1.0->sktime==0.9.0) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.8/site-packages (from pandas>=1.1.0->sktime==0.9.0) (2021.3)\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.8/site-packages (from python-dateutil>=2.8.1->pandas>=1.1.0->sktime==0.9.0) (1.16.0)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /opt/conda/lib/python3.8/site-packages (from scikit-learn>=0.24.0->sktime==0.9.0) (3.0.0)\n",
      "Requirement already satisfied: scipy>=1.1.0 in /opt/conda/lib/python3.8/site-packages (from scikit-learn>=0.24.0->sktime==0.9.0) (1.6.3)\n",
      "Requirement already satisfied: joblib>=0.11 in /opt/conda/lib/python3.8/site-packages (from scikit-learn>=0.24.0->sktime==0.9.0) (1.1.0)\n",
      "Requirement already satisfied: patsy>=0.5 in /opt/conda/lib/python3.8/site-packages (from statsmodels>=0.12.1->sktime==0.9.0) (0.5.2)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n",
      "Looking in indexes: https://pypi.org/simple, https://pypi.ngc.nvidia.com\n",
      "Collecting numpy==1.20\n",
      "  Downloading numpy-1.20.0-cp38-cp38-manylinux2010_x86_64.whl (15.4 MB)\n",
      "\u001b[K     |████████████████████████████████| 15.4 MB 12.8 MB/s eta 0:00:01\n",
      "\u001b[?25hInstalling collected packages: numpy\n",
      "  Attempting uninstall: numpy\n",
      "    Found existing installation: numpy 1.19.3\n",
      "    Uninstalling numpy-1.19.3:\n",
      "      Successfully uninstalled numpy-1.19.3\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "sktime 0.9.0 requires numpy<=1.19.3, but you have numpy 1.20.0 which is incompatible.\u001b[0m\n",
      "Successfully installed numpy-1.20.0\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n",
      "Looking in indexes: https://pypi.org/simple, https://pypi.ngc.nvidia.com\n",
      "Collecting wandb\n",
      "  Downloading wandb-0.12.10-py2.py3-none-any.whl (1.7 MB)\n",
      "\u001b[K     |████████████████████████████████| 1.7 MB 22.7 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting shortuuid>=0.5.0\n",
      "  Downloading shortuuid-1.0.8-py3-none-any.whl (9.5 kB)\n",
      "Collecting promise<3,>=2.0\n",
      "  Downloading promise-2.3.tar.gz (19 kB)\n",
      "Requirement already satisfied: psutil>=5.0.0 in /opt/conda/lib/python3.8/site-packages (from wandb) (5.8.0)\n",
      "Requirement already satisfied: six>=1.13.0 in /opt/conda/lib/python3.8/site-packages (from wandb) (1.16.0)\n",
      "Requirement already satisfied: PyYAML in /opt/conda/lib/python3.8/site-packages (from wandb) (5.4.1)\n",
      "Requirement already satisfied: requests<3,>=2.0.0 in /opt/conda/lib/python3.8/site-packages (from wandb) (2.26.0)\n",
      "Collecting pathtools\n",
      "  Downloading pathtools-0.1.2.tar.gz (11 kB)\n",
      "Collecting GitPython>=1.0.0\n",
      "  Downloading GitPython-3.1.26-py3-none-any.whl (180 kB)\n",
      "\u001b[K     |████████████████████████████████| 180 kB 30.4 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting docker-pycreds>=0.4.0\n",
      "  Downloading docker_pycreds-0.4.0-py2.py3-none-any.whl (9.0 kB)\n",
      "Collecting sentry-sdk>=1.0.0\n",
      "  Downloading sentry_sdk-1.5.5-py2.py3-none-any.whl (144 kB)\n",
      "\u001b[K     |████████████████████████████████| 144 kB 29.2 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting yaspin>=1.0.0\n",
      "  Downloading yaspin-2.1.0-py3-none-any.whl (18 kB)\n",
      "Collecting protobuf>=3.12.0\n",
      "  Downloading protobuf-3.19.4-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.1 MB)\n",
      "\u001b[K     |████████████████████████████████| 1.1 MB 55.9 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: python-dateutil>=2.6.1 in /opt/conda/lib/python3.8/site-packages (from wandb) (2.8.2)\n",
      "Requirement already satisfied: Click!=8.0.0,>=7.0 in /opt/conda/lib/python3.8/site-packages (from wandb) (8.0.1)\n",
      "Collecting gitdb<5,>=4.0.1\n",
      "  Downloading gitdb-4.0.9-py3-none-any.whl (63 kB)\n",
      "\u001b[K     |████████████████████████████████| 63 kB 36.9 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting smmap<6,>=3.0.1\n",
      "  Downloading smmap-5.0.0-py3-none-any.whl (24 kB)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.8/site-packages (from requests<3,>=2.0.0->wandb) (3.1)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.8/site-packages (from requests<3,>=2.0.0->wandb) (1.26.7)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /opt/conda/lib/python3.8/site-packages (from requests<3,>=2.0.0->wandb) (2.0.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.8/site-packages (from requests<3,>=2.0.0->wandb) (2021.5.30)\n",
      "Collecting termcolor<2.0.0,>=1.1.0\n",
      "  Downloading termcolor-1.1.0.tar.gz (3.9 kB)\n",
      "Building wheels for collected packages: promise, termcolor, pathtools\n",
      "  Building wheel for promise (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for promise: filename=promise-2.3-py3-none-any.whl size=21502 sha256=4a9f5dc074ef75e7367d35cfe3fd2d1d76d8285461291624710330797aa72167\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-edf97i22/wheels/54/aa/01/724885182f93150035a2a91bce34a12877e8067a97baaf5dc8\n",
      "  Building wheel for termcolor (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for termcolor: filename=termcolor-1.1.0-py3-none-any.whl size=4847 sha256=53fa3d0a0e6686f85b6284bae7326e9c291880c1698495ff0c5a9908ffed128b\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-edf97i22/wheels/a0/16/9c/5473df82468f958445479c59e784896fa24f4a5fc024b0f501\n",
      "  Building wheel for pathtools (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for pathtools: filename=pathtools-0.1.2-py3-none-any.whl size=8807 sha256=874b92a4306710f7a9721ad0c4f73b3b6b80df3db9c4d52be8c6908be4d59bed\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-edf97i22/wheels/4c/8e/7e/72fbc243e1aeecae64a96875432e70d4e92f3d2d18123be004\n",
      "Successfully built promise termcolor pathtools\n",
      "Installing collected packages: smmap, termcolor, gitdb, yaspin, shortuuid, sentry-sdk, protobuf, promise, pathtools, GitPython, docker-pycreds, wandb\n",
      "  Attempting uninstall: protobuf\n",
      "    Found existing installation: protobuf 3.11.2\n",
      "    Uninstalling protobuf-3.11.2:\n",
      "      Successfully uninstalled protobuf-3.11.2\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "matrixprofile 1.1.10 requires protobuf==3.11.2, but you have protobuf 3.19.4 which is incompatible.\u001b[0m\n",
      "Successfully installed GitPython-3.1.26 docker-pycreds-0.4.0 gitdb-4.0.9 pathtools-0.1.2 promise-2.3 protobuf-3.19.4 sentry-sdk-1.5.5 shortuuid-1.0.8 smmap-5.0.0 termcolor-1.1.0 wandb-0.12.10 yaspin-2.1.0\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n",
      "Looking in indexes: https://pypi.org/simple, https://pypi.ngc.nvidia.com\n",
      "Collecting optuna\n",
      "  Downloading optuna-2.10.0-py3-none-any.whl (308 kB)\n",
      "\u001b[K     |████████████████████████████████| 308 kB 20.0 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting sqlalchemy>=1.1.0\n",
      "  Downloading SQLAlchemy-1.4.31-cp38-cp38-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.6 MB)\n",
      "\u001b[K     |████████████████████████████████| 1.6 MB 29.5 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting cmaes>=0.8.2\n",
      "  Downloading cmaes-0.8.2-py3-none-any.whl (15 kB)\n",
      "Collecting colorlog\n",
      "  Downloading colorlog-6.6.0-py2.py3-none-any.whl (11 kB)\n",
      "Requirement already satisfied: scipy!=1.4.0 in /opt/conda/lib/python3.8/site-packages (from optuna) (1.6.3)\n",
      "Requirement already satisfied: PyYAML in /opt/conda/lib/python3.8/site-packages (from optuna) (5.4.1)\n",
      "Collecting alembic\n",
      "  Downloading alembic-1.7.6-py3-none-any.whl (210 kB)\n",
      "\u001b[K     |████████████████████████████████| 210 kB 27.1 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting cliff\n",
      "  Downloading cliff-3.10.0-py3-none-any.whl (80 kB)\n",
      "\u001b[K     |████████████████████████████████| 80 kB 41.0 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: tqdm in /opt/conda/lib/python3.8/site-packages (from optuna) (4.62.3)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.8/site-packages (from optuna) (21.0)\n",
      "Requirement already satisfied: numpy in /opt/conda/lib/python3.8/site-packages (from optuna) (1.20.0)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in /opt/conda/lib/python3.8/site-packages (from packaging>=20.0->optuna) (2.4.7)\n",
      "Collecting greenlet!=0.4.17\n",
      "  Downloading greenlet-1.1.2-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (156 kB)\n",
      "\u001b[K     |████████████████████████████████| 156 kB 37.0 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting importlib-metadata\n",
      "  Downloading importlib_metadata-4.11.0-py3-none-any.whl (17 kB)\n",
      "Collecting Mako\n",
      "  Downloading Mako-1.1.6-py2.py3-none-any.whl (75 kB)\n",
      "\u001b[K     |████████████████████████████████| 75 kB 37.0 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting importlib-resources\n",
      "  Downloading importlib_resources-5.4.0-py3-none-any.whl (28 kB)\n",
      "Collecting autopage>=0.4.0\n",
      "  Downloading autopage-0.5.0-py3-none-any.whl (29 kB)\n",
      "Collecting cmd2>=1.0.0\n",
      "  Downloading cmd2-2.3.3-py3-none-any.whl (149 kB)\n",
      "\u001b[K     |████████████████████████████████| 149 kB 28.6 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting pbr!=2.1.0,>=2.0.0\n",
      "  Downloading pbr-5.8.1-py2.py3-none-any.whl (113 kB)\n",
      "\u001b[K     |████████████████████████████████| 113 kB 29.3 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: PrettyTable>=0.7.2 in /opt/conda/lib/python3.8/site-packages (from cliff->optuna) (2.2.1)\n",
      "Collecting stevedore>=2.0.1\n",
      "  Downloading stevedore-3.5.0-py3-none-any.whl (49 kB)\n",
      "\u001b[K     |████████████████████████████████| 49 kB 37.8 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting pyperclip>=1.6\n",
      "  Downloading pyperclip-1.8.2.tar.gz (20 kB)\n",
      "Requirement already satisfied: attrs>=16.3.0 in /opt/conda/lib/python3.8/site-packages (from cmd2>=1.0.0->cliff->optuna) (21.2.0)\n",
      "Requirement already satisfied: wcwidth>=0.1.7 in /opt/conda/lib/python3.8/site-packages (from cmd2>=1.0.0->cliff->optuna) (0.2.5)\n",
      "Collecting zipp>=0.5\n",
      "  Downloading zipp-3.7.0-py3-none-any.whl (5.3 kB)\n",
      "Requirement already satisfied: MarkupSafe>=0.9.2 in /opt/conda/lib/python3.8/site-packages (from Mako->alembic->optuna) (2.0.1)\n",
      "Building wheels for collected packages: pyperclip\n",
      "  Building wheel for pyperclip (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for pyperclip: filename=pyperclip-1.8.2-py3-none-any.whl size=11136 sha256=5722bebe4b8edca9ec48cffbd48203985bd9e0bf626521ba3064b3e68cf232c8\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-i_00hnzv/wheels/7f/1a/65/84ff8c386bec21fca6d220ea1f5498a0367883a78dd5ba6122\n",
      "Successfully built pyperclip\n",
      "Installing collected packages: zipp, pyperclip, pbr, greenlet, stevedore, sqlalchemy, Mako, importlib-resources, importlib-metadata, cmd2, autopage, colorlog, cmaes, cliff, alembic, optuna\n",
      "Successfully installed Mako-1.1.6 alembic-1.7.6 autopage-0.5.0 cliff-3.10.0 cmaes-0.8.2 cmd2-2.3.3 colorlog-6.6.0 greenlet-1.1.2 importlib-metadata-4.11.0 importlib-resources-5.4.0 optuna-2.10.0 pbr-5.8.1 pyperclip-1.8.2 sqlalchemy-1.4.31 stevedore-3.5.0 zipp-3.7.0\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install llvmlite==0.38.0 --ignore-installed\n",
    "!pip install tsai -U >> /dev/null\n",
    "!pip install --upgrade sktime==0.9.0\n",
    "!pip3 install --upgrade numpy==1.20\n",
    "!pip install wandb -U\n",
    "!pip install optuna -U"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3d0a478d",
   "metadata": {
    "gradient": {
     "editing": false,
     "execution_count": 1,
     "id": "f4a680db-74ef-4660-877c-83ecb4895ef1",
     "kernelId": "e731389a-9397-4fee-9c6b-efa8de72a05c"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "OMP: Info #271: omp_set_nested routine deprecated, please use omp_set_max_active_levels instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "os             : Linux-5.4.0-89-generic-x86_64-with-glibc2.10\n",
      "python         : 3.8.12\n",
      "tsai           : 0.2.25\n",
      "fastai         : 2.5.3\n",
      "fastcore       : 1.3.27\n",
      "wandb          : 0.12.10\n",
      "torch          : 1.10.0a0+0aef44c\n",
      "n_cpus         : 8\n",
      "device         : cuda (Quadro RTX 4000)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from tsai.all import *\n",
    "from scipy import io\n",
    "import inception_time_model as itm\n",
    "import wandb\n",
    "from fastai.callback.wandb import *\n",
    "from getdata import get_fofu_data\n",
    "my_setup(wandb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "14c82727",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33msashalikesplanes\u001b[0m (use `wandb login --relogin` to force relogin)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wandb.login()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa6304e1",
   "metadata": {},
   "source": [
    "Define the sweep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "45e9ed49",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Create sweep with ID: 7g0f0hky\n",
      "Sweep URL: https://wandb.ai/sashalikesplanes/HOClass/sweeps/7g0f0hky\n"
     ]
    }
   ],
   "source": [
    "sweep_config = {\n",
    "    \"name\" : \"Sweep on LR, Wd, and Dropout using Standardized data with accuracy as metric\",\n",
    "    \"project\" : \"HOClass\",\n",
    "    \"description\" : \"A sweep using Bayes search, adjusted range on Wd, decreased lower bound on Conv_dropout factor, added data standardization using Verskeer method, changed bottleneck to always off, switched residual to always off, switched LR to use a range\",\n",
    "    \"method\" : \"bayes\",\n",
    "    \"metric\": {\n",
    "        \"name\": \"accuracy\",\n",
    "        \"goal\": \"maximize\",\n",
    "    },\n",
    "    \"early_terminate\" : {\n",
    "        \"type\" : \"hyperband\",\n",
    "        \"min_iter\" : 7500,\n",
    "        \"eta\" : 2,\n",
    "    },\n",
    "  \n",
    "    \"parameters\" : {\n",
    "        \"epochs\" : {\n",
    "            \"value\" : 75\n",
    "        },\n",
    "        \"lr\" : {\n",
    "            \"min\": 0.001,\n",
    "            \"max\" : 0.025,\n",
    "        },\n",
    "        \"batch_size\" : {\n",
    "            \"value\" : 64\n",
    "        },\n",
    "        \"nf\" : {\n",
    "            \"value\" : 24,\n",
    "        },\n",
    "        \"ks\" : {\n",
    "            \"value\" : 64\n",
    "        },\n",
    "        \"bottleneck\" : {\n",
    "            \"value\" : False\n",
    "        },\n",
    "        \"bottleneck_size\"  : {\n",
    "            \"value\" : 0\n",
    "        },\n",
    "        \"depth\"  : {\n",
    "            \"value\" :  6\n",
    "        },\n",
    "        \"residual\"  : {\n",
    "            \"value\" : False\n",
    "        },\n",
    "        \"valid_pct\"  : {\n",
    "            \"value\" : 0.2\n",
    "        },\n",
    "        \"wd\" : {\n",
    "            \"distribution\" : \"log_uniform\",\n",
    "            \"min\" : -4.5,\n",
    "            \"max\" : -0.5,\n",
    "        },\n",
    "        \"conv_dropout\" : {\n",
    "            \"min\" : 0.1,\n",
    "            \"max\" : 0.75,\n",
    "        },\n",
    "        \"variables\" : {\n",
    "            \"value\" : ['e', 'u', 'x', 'dedt', 'dudt', 'dxdt']\n",
    "        },\n",
    "        \"bn\" : {\n",
    "            \"values\" : [True, False]\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "sweep_id = wandb.sweep(sweep_config, project=\"HOClass\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76b5fc20",
   "metadata": {},
   "source": [
    "Current sweep id : 7g0f0hky"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "91b6fb8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "sweep_id = \"7g0f0hky\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "580e8e8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train():\n",
    "    with wandb.init(project=\"HOClass\") as run:\n",
    "        config = wandb.config\n",
    "        # Get the data sets\n",
    "        print(config)\n",
    "        X, Y, splits = get_fofu_data(valid_pct=config[\"valid_pct\"])\n",
    "        print(Y)\n",
    "        tfms = [None, Categorize()]\n",
    "        dsets = TSDatasets(X, Y, tfms=tfms, splits=splits, inplace=True)\n",
    "        \n",
    "        # Get data loaders\n",
    "        batch_tfms = None #[TSStandardize(by_var=config[\"standardize_by_var\"], by_sample=config[\"standardize_by_sample\"])]\n",
    "        dls = TSDataLoaders.from_dsets(dsets.train, dsets.valid, bs=config[\"batch_size\"], batch_tfms=batch_tfms, shuffle=True, num_workers=8)\n",
    "        \n",
    "        # Get model\n",
    "        cbs = [SaveModelCallback(), WandbCallback(log_preds=True, log_model=True, dataset_name='valid runs random, standardized')]\n",
    "        model = InceptionTimePlus(dls.vars, dls.c, nf=config[\"nf\"], bn=config[\"bn\"], ks=config[\"ks\"], bottleneck=config[\"bottleneck\"],\n",
    "                                  conv_dropout=config[\"conv_dropout\"], depth=config[\"depth\"])\n",
    "        # Get learner\n",
    "        learner = Learner(dls, model, opt_func=Adam, metrics=accuracy, wd=config[\"wd\"], cbs=cbs)\n",
    "        # Execute fit one cycle training\n",
    "        learner.fit_one_cycle(config[\"epochs\"], lr_max=config[\"lr\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9eb0a82e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: bpp4onpk with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 64\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbn: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbottleneck: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbottleneck_size: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tconv_dropout: 0.5225135195072143\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdepth: 6\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepochs: 75\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tks: 64\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlr: 0.01000788060963436\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnf: 24\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tresidual: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tvalid_pct: 0.2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tvariables: ['e', 'u', 'x', 'dedt', 'dudt', 'dxdt']\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \twd: 0.0986512165707266\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33msashalikesplanes\u001b[0m (use `wandb login --relogin` to force relogin)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Ignored wandb.init() arg project when running a sweep.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                    Syncing run <strong><a href=\"https://wandb.ai/sashalikesplanes/HOClass/runs/bpp4onpk\" target=\"_blank\">stellar-sweep-20</a></strong> to <a href=\"https://wandb.ai/sashalikesplanes/HOClass\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">docs</a>).<br/>\n",
       "Sweep page: <a href=\"https://wandb.ai/sashalikesplanes/HOClass/sweeps/7g0f0hky\" target=\"_blank\">https://wandb.ai/sashalikesplanes/HOClass/sweeps/7g0f0hky</a><br/>\n",
       "\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'batch_size': 64, 'bn': True, 'bottleneck': False, 'bottleneck_size': 0, 'conv_dropout': 0.5225135195072143, 'depth': 6, 'epochs': 75, 'ks': 64, 'lr': 0.01000788060963436, 'nf': 24, 'residual': False, 'valid_pct': 0.2, 'variables': ['e', 'u', 'x', 'dedt', 'dudt', 'dxdt'], 'wd': 0.0986512165707266}\n",
      "[0. 0. 0. ... 2. 2. 2.]\n",
      "WandbCallback was not able to prepare a DataLoader for logging prediction samples -> Expected an input of type in \n",
      "  - <class 'numpy.float64'>\n",
      "  - <class 'fastai.torch_core.TensorCategory'>\n",
      " but got <class 'numpy.ndarray'>\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.534254</td>\n",
       "      <td>0.759742</td>\n",
       "      <td>0.636786</td>\n",
       "      <td>00:32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.465511</td>\n",
       "      <td>1.044670</td>\n",
       "      <td>0.524458</td>\n",
       "      <td>00:30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.386665</td>\n",
       "      <td>1.391679</td>\n",
       "      <td>0.474360</td>\n",
       "      <td>00:30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.387879</td>\n",
       "      <td>1.577527</td>\n",
       "      <td>0.512800</td>\n",
       "      <td>00:30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.368454</td>\n",
       "      <td>1.724442</td>\n",
       "      <td>0.523356</td>\n",
       "      <td>00:30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.355011</td>\n",
       "      <td>1.382772</td>\n",
       "      <td>0.555494</td>\n",
       "      <td>00:31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.337568</td>\n",
       "      <td>1.377158</td>\n",
       "      <td>0.604726</td>\n",
       "      <td>00:32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.344892</td>\n",
       "      <td>1.321904</td>\n",
       "      <td>0.586766</td>\n",
       "      <td>00:31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.329161</td>\n",
       "      <td>1.826727</td>\n",
       "      <td>0.574478</td>\n",
       "      <td>00:31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.320924</td>\n",
       "      <td>2.092039</td>\n",
       "      <td>0.557148</td>\n",
       "      <td>00:31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.334071</td>\n",
       "      <td>1.291880</td>\n",
       "      <td>0.600788</td>\n",
       "      <td>00:31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>0.349231</td>\n",
       "      <td>1.349967</td>\n",
       "      <td>0.609059</td>\n",
       "      <td>00:31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>0.332264</td>\n",
       "      <td>1.486243</td>\n",
       "      <td>0.615912</td>\n",
       "      <td>00:31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>0.328317</td>\n",
       "      <td>1.624799</td>\n",
       "      <td>0.568728</td>\n",
       "      <td>00:30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>0.342863</td>\n",
       "      <td>1.303521</td>\n",
       "      <td>0.580465</td>\n",
       "      <td>00:31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>0.341719</td>\n",
       "      <td>1.632284</td>\n",
       "      <td>0.593462</td>\n",
       "      <td>00:32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>0.327130</td>\n",
       "      <td>0.978162</td>\n",
       "      <td>0.640173</td>\n",
       "      <td>00:31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>0.335771</td>\n",
       "      <td>1.326716</td>\n",
       "      <td>0.589523</td>\n",
       "      <td>00:32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>0.335714</td>\n",
       "      <td>1.948139</td>\n",
       "      <td>0.552974</td>\n",
       "      <td>00:31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>0.344829</td>\n",
       "      <td>1.115535</td>\n",
       "      <td>0.644506</td>\n",
       "      <td>00:30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>0.323862</td>\n",
       "      <td>1.114346</td>\n",
       "      <td>0.623080</td>\n",
       "      <td>00:31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21</td>\n",
       "      <td>0.334500</td>\n",
       "      <td>0.957039</td>\n",
       "      <td>0.640567</td>\n",
       "      <td>00:31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22</td>\n",
       "      <td>0.328800</td>\n",
       "      <td>1.096119</td>\n",
       "      <td>0.600866</td>\n",
       "      <td>00:32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23</td>\n",
       "      <td>0.309366</td>\n",
       "      <td>0.849709</td>\n",
       "      <td>0.644033</td>\n",
       "      <td>00:31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24</td>\n",
       "      <td>0.303398</td>\n",
       "      <td>1.512618</td>\n",
       "      <td>0.580071</td>\n",
       "      <td>00:31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25</td>\n",
       "      <td>0.322420</td>\n",
       "      <td>1.148199</td>\n",
       "      <td>0.634581</td>\n",
       "      <td>00:31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26</td>\n",
       "      <td>0.305158</td>\n",
       "      <td>1.144527</td>\n",
       "      <td>0.637022</td>\n",
       "      <td>00:33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27</td>\n",
       "      <td>0.317171</td>\n",
       "      <td>1.392392</td>\n",
       "      <td>0.595353</td>\n",
       "      <td>00:31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28</td>\n",
       "      <td>0.316507</td>\n",
       "      <td>1.123632</td>\n",
       "      <td>0.595037</td>\n",
       "      <td>00:31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29</td>\n",
       "      <td>0.309338</td>\n",
       "      <td>1.557409</td>\n",
       "      <td>0.592911</td>\n",
       "      <td>00:32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>0.320927</td>\n",
       "      <td>0.965431</td>\n",
       "      <td>0.653092</td>\n",
       "      <td>00:33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>31</td>\n",
       "      <td>0.312605</td>\n",
       "      <td>1.374454</td>\n",
       "      <td>0.596692</td>\n",
       "      <td>00:32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>32</td>\n",
       "      <td>0.320394</td>\n",
       "      <td>1.030700</td>\n",
       "      <td>0.633478</td>\n",
       "      <td>00:31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>33</td>\n",
       "      <td>0.295557</td>\n",
       "      <td>1.039062</td>\n",
       "      <td>0.623316</td>\n",
       "      <td>00:32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>34</td>\n",
       "      <td>0.293030</td>\n",
       "      <td>1.189860</td>\n",
       "      <td>0.631824</td>\n",
       "      <td>00:32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>35</td>\n",
       "      <td>0.294489</td>\n",
       "      <td>1.176895</td>\n",
       "      <td>0.618511</td>\n",
       "      <td>00:31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>36</td>\n",
       "      <td>0.285211</td>\n",
       "      <td>1.384439</td>\n",
       "      <td>0.619220</td>\n",
       "      <td>00:33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>37</td>\n",
       "      <td>0.295577</td>\n",
       "      <td>1.092030</td>\n",
       "      <td>0.618117</td>\n",
       "      <td>00:32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>38</td>\n",
       "      <td>0.290901</td>\n",
       "      <td>1.009983</td>\n",
       "      <td>0.622686</td>\n",
       "      <td>00:31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>39</td>\n",
       "      <td>0.280151</td>\n",
       "      <td>1.625064</td>\n",
       "      <td>0.561087</td>\n",
       "      <td>00:32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>0.282762</td>\n",
       "      <td>0.769901</td>\n",
       "      <td>0.672469</td>\n",
       "      <td>00:31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>41</td>\n",
       "      <td>0.266957</td>\n",
       "      <td>1.062097</td>\n",
       "      <td>0.636865</td>\n",
       "      <td>00:31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>42</td>\n",
       "      <td>0.260557</td>\n",
       "      <td>0.934651</td>\n",
       "      <td>0.651910</td>\n",
       "      <td>00:31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>43</td>\n",
       "      <td>0.265195</td>\n",
       "      <td>1.161790</td>\n",
       "      <td>0.613312</td>\n",
       "      <td>00:32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>44</td>\n",
       "      <td>0.251772</td>\n",
       "      <td>1.276438</td>\n",
       "      <td>0.596534</td>\n",
       "      <td>00:32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>45</td>\n",
       "      <td>0.246550</td>\n",
       "      <td>1.399726</td>\n",
       "      <td>0.638440</td>\n",
       "      <td>00:31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>46</td>\n",
       "      <td>0.247026</td>\n",
       "      <td>1.105659</td>\n",
       "      <td>0.607483</td>\n",
       "      <td>00:31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>47</td>\n",
       "      <td>0.235275</td>\n",
       "      <td>1.228765</td>\n",
       "      <td>0.620402</td>\n",
       "      <td>00:31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>48</td>\n",
       "      <td>0.232308</td>\n",
       "      <td>1.409110</td>\n",
       "      <td>0.617015</td>\n",
       "      <td>00:32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>49</td>\n",
       "      <td>0.216380</td>\n",
       "      <td>1.257311</td>\n",
       "      <td>0.616463</td>\n",
       "      <td>00:32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>0.220398</td>\n",
       "      <td>1.564215</td>\n",
       "      <td>0.598031</td>\n",
       "      <td>00:32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>51</td>\n",
       "      <td>0.223421</td>\n",
       "      <td>1.451591</td>\n",
       "      <td>0.607011</td>\n",
       "      <td>00:32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>52</td>\n",
       "      <td>0.206101</td>\n",
       "      <td>1.503072</td>\n",
       "      <td>0.605514</td>\n",
       "      <td>00:32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>53</td>\n",
       "      <td>0.199079</td>\n",
       "      <td>1.133842</td>\n",
       "      <td>0.642379</td>\n",
       "      <td>00:32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>54</td>\n",
       "      <td>0.209204</td>\n",
       "      <td>1.332819</td>\n",
       "      <td>0.620874</td>\n",
       "      <td>00:31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>55</td>\n",
       "      <td>0.185792</td>\n",
       "      <td>1.737177</td>\n",
       "      <td>0.575030</td>\n",
       "      <td>00:31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>56</td>\n",
       "      <td>0.176107</td>\n",
       "      <td>1.691463</td>\n",
       "      <td>0.599212</td>\n",
       "      <td>00:31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>57</td>\n",
       "      <td>0.180360</td>\n",
       "      <td>1.292346</td>\n",
       "      <td>0.611658</td>\n",
       "      <td>00:31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>58</td>\n",
       "      <td>0.150146</td>\n",
       "      <td>1.328252</td>\n",
       "      <td>0.624577</td>\n",
       "      <td>00:31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>59</td>\n",
       "      <td>0.156140</td>\n",
       "      <td>1.388211</td>\n",
       "      <td>0.632611</td>\n",
       "      <td>00:31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>0.149607</td>\n",
       "      <td>1.678097</td>\n",
       "      <td>0.582119</td>\n",
       "      <td>00:31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>61</td>\n",
       "      <td>0.142000</td>\n",
       "      <td>1.521880</td>\n",
       "      <td>0.616384</td>\n",
       "      <td>00:32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>62</td>\n",
       "      <td>0.121889</td>\n",
       "      <td>1.243792</td>\n",
       "      <td>0.643639</td>\n",
       "      <td>00:32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>63</td>\n",
       "      <td>0.107215</td>\n",
       "      <td>1.316691</td>\n",
       "      <td>0.637495</td>\n",
       "      <td>00:31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>64</td>\n",
       "      <td>0.094843</td>\n",
       "      <td>1.334758</td>\n",
       "      <td>0.636707</td>\n",
       "      <td>00:32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>65</td>\n",
       "      <td>0.114359</td>\n",
       "      <td>1.435078</td>\n",
       "      <td>0.632532</td>\n",
       "      <td>00:31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>66</td>\n",
       "      <td>0.091538</td>\n",
       "      <td>1.433819</td>\n",
       "      <td>0.626467</td>\n",
       "      <td>00:31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>67</td>\n",
       "      <td>0.084950</td>\n",
       "      <td>1.423054</td>\n",
       "      <td>0.638598</td>\n",
       "      <td>00:31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>68</td>\n",
       "      <td>0.074309</td>\n",
       "      <td>1.431836</td>\n",
       "      <td>0.637731</td>\n",
       "      <td>00:32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>69</td>\n",
       "      <td>0.066543</td>\n",
       "      <td>1.625943</td>\n",
       "      <td>0.628515</td>\n",
       "      <td>00:31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>70</td>\n",
       "      <td>0.066221</td>\n",
       "      <td>1.629099</td>\n",
       "      <td>0.626310</td>\n",
       "      <td>00:32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>71</td>\n",
       "      <td>0.060755</td>\n",
       "      <td>1.571393</td>\n",
       "      <td>0.618432</td>\n",
       "      <td>00:31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>72</td>\n",
       "      <td>0.055766</td>\n",
       "      <td>1.590281</td>\n",
       "      <td>0.627097</td>\n",
       "      <td>00:31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>73</td>\n",
       "      <td>0.054875</td>\n",
       "      <td>1.619202</td>\n",
       "      <td>0.625522</td>\n",
       "      <td>00:31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>74</td>\n",
       "      <td>0.050616</td>\n",
       "      <td>1.495523</td>\n",
       "      <td>0.642300</td>\n",
       "      <td>00:32</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Better model found at epoch 0 with valid_loss value: 0.7597423791885376.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<br/>Waiting for W&B process to finish, PID 1222... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "wandb.agent(sweep_id, project=\"HOClass\", function=train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "5a6adfc5",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "                    Syncing run <strong><a href=\"https://wandb.ai/sashalikesplanes/HOClass/runs/2y7jk59p\" target=\"_blank\">All vars, dropout=0.1</a></strong> to <a href=\"https://wandb.ai/sashalikesplanes/HOClass\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">docs</a>).<br/>\n",
       "\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WandbCallback was not able to prepare a DataLoader for logging prediction samples -> Expected an input of type in \n",
      "  - <class 'numpy.float64'>\n",
      "  - <class 'fastai.torch_core.TensorCategory'>\n",
      " but got <class 'numpy.ndarray'>\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.693357</td>\n",
       "      <td>0.679048</td>\n",
       "      <td>0.689326</td>\n",
       "      <td>00:33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.643246</td>\n",
       "      <td>0.697719</td>\n",
       "      <td>0.680731</td>\n",
       "      <td>00:30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.632047</td>\n",
       "      <td>0.630845</td>\n",
       "      <td>0.714565</td>\n",
       "      <td>00:31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.605246</td>\n",
       "      <td>0.615372</td>\n",
       "      <td>0.721675</td>\n",
       "      <td>00:31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.583077</td>\n",
       "      <td>0.607768</td>\n",
       "      <td>0.724879</td>\n",
       "      <td>00:31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.574802</td>\n",
       "      <td>0.621701</td>\n",
       "      <td>0.725270</td>\n",
       "      <td>00:31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.561086</td>\n",
       "      <td>0.559472</td>\n",
       "      <td>0.748398</td>\n",
       "      <td>00:31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.546220</td>\n",
       "      <td>0.546934</td>\n",
       "      <td>0.758400</td>\n",
       "      <td>00:31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.527859</td>\n",
       "      <td>0.567852</td>\n",
       "      <td>0.748789</td>\n",
       "      <td>00:32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.518102</td>\n",
       "      <td>0.535232</td>\n",
       "      <td>0.766604</td>\n",
       "      <td>00:32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.489228</td>\n",
       "      <td>0.516300</td>\n",
       "      <td>0.771839</td>\n",
       "      <td>00:32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>0.464804</td>\n",
       "      <td>0.527555</td>\n",
       "      <td>0.770980</td>\n",
       "      <td>00:31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>0.458069</td>\n",
       "      <td>0.481247</td>\n",
       "      <td>0.792233</td>\n",
       "      <td>00:31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>0.435025</td>\n",
       "      <td>0.527378</td>\n",
       "      <td>0.773090</td>\n",
       "      <td>00:31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>0.428287</td>\n",
       "      <td>0.484599</td>\n",
       "      <td>0.792233</td>\n",
       "      <td>00:33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>0.411029</td>\n",
       "      <td>0.477255</td>\n",
       "      <td>0.794030</td>\n",
       "      <td>00:33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>0.398744</td>\n",
       "      <td>0.474553</td>\n",
       "      <td>0.797390</td>\n",
       "      <td>00:32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>0.372345</td>\n",
       "      <td>0.498747</td>\n",
       "      <td>0.790123</td>\n",
       "      <td>00:32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>0.369769</td>\n",
       "      <td>0.486055</td>\n",
       "      <td>0.800985</td>\n",
       "      <td>00:31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>0.333214</td>\n",
       "      <td>0.467233</td>\n",
       "      <td>0.804735</td>\n",
       "      <td>00:32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>0.357529</td>\n",
       "      <td>0.482620</td>\n",
       "      <td>0.795749</td>\n",
       "      <td>00:32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21</td>\n",
       "      <td>0.318060</td>\n",
       "      <td>0.471996</td>\n",
       "      <td>0.806767</td>\n",
       "      <td>00:32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22</td>\n",
       "      <td>0.301957</td>\n",
       "      <td>0.511975</td>\n",
       "      <td>0.795046</td>\n",
       "      <td>00:32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23</td>\n",
       "      <td>0.279500</td>\n",
       "      <td>0.504461</td>\n",
       "      <td>0.800438</td>\n",
       "      <td>00:33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24</td>\n",
       "      <td>0.271586</td>\n",
       "      <td>0.551572</td>\n",
       "      <td>0.790123</td>\n",
       "      <td>00:32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25</td>\n",
       "      <td>0.255777</td>\n",
       "      <td>0.552581</td>\n",
       "      <td>0.801844</td>\n",
       "      <td>00:31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26</td>\n",
       "      <td>0.223307</td>\n",
       "      <td>0.570906</td>\n",
       "      <td>0.790436</td>\n",
       "      <td>00:31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27</td>\n",
       "      <td>0.202932</td>\n",
       "      <td>0.595789</td>\n",
       "      <td>0.796531</td>\n",
       "      <td>00:33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28</td>\n",
       "      <td>0.203084</td>\n",
       "      <td>0.593943</td>\n",
       "      <td>0.789030</td>\n",
       "      <td>00:31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29</td>\n",
       "      <td>0.185731</td>\n",
       "      <td>0.596985</td>\n",
       "      <td>0.792780</td>\n",
       "      <td>00:33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>0.158634</td>\n",
       "      <td>0.601890</td>\n",
       "      <td>0.795359</td>\n",
       "      <td>00:31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>31</td>\n",
       "      <td>0.161413</td>\n",
       "      <td>0.626309</td>\n",
       "      <td>0.790905</td>\n",
       "      <td>00:32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>32</td>\n",
       "      <td>0.143742</td>\n",
       "      <td>0.629889</td>\n",
       "      <td>0.799656</td>\n",
       "      <td>00:31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>33</td>\n",
       "      <td>0.118370</td>\n",
       "      <td>0.620835</td>\n",
       "      <td>0.802860</td>\n",
       "      <td>00:31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>34</td>\n",
       "      <td>0.104415</td>\n",
       "      <td>0.690287</td>\n",
       "      <td>0.799344</td>\n",
       "      <td>00:33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>35</td>\n",
       "      <td>0.095774</td>\n",
       "      <td>0.719892</td>\n",
       "      <td>0.791608</td>\n",
       "      <td>00:32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>36</td>\n",
       "      <td>0.086755</td>\n",
       "      <td>0.728997</td>\n",
       "      <td>0.796140</td>\n",
       "      <td>00:32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>37</td>\n",
       "      <td>0.077018</td>\n",
       "      <td>0.753659</td>\n",
       "      <td>0.794265</td>\n",
       "      <td>00:33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>38</td>\n",
       "      <td>0.072345</td>\n",
       "      <td>0.778262</td>\n",
       "      <td>0.793952</td>\n",
       "      <td>00:34</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>39</td>\n",
       "      <td>0.065246</td>\n",
       "      <td>0.765150</td>\n",
       "      <td>0.797000</td>\n",
       "      <td>00:31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>0.061837</td>\n",
       "      <td>0.797460</td>\n",
       "      <td>0.793796</td>\n",
       "      <td>00:32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>41</td>\n",
       "      <td>0.054196</td>\n",
       "      <td>0.796638</td>\n",
       "      <td>0.796687</td>\n",
       "      <td>00:32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>42</td>\n",
       "      <td>0.038567</td>\n",
       "      <td>0.811775</td>\n",
       "      <td>0.795906</td>\n",
       "      <td>00:32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>43</td>\n",
       "      <td>0.042165</td>\n",
       "      <td>0.841409</td>\n",
       "      <td>0.795827</td>\n",
       "      <td>00:33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>44</td>\n",
       "      <td>0.042326</td>\n",
       "      <td>0.832049</td>\n",
       "      <td>0.797156</td>\n",
       "      <td>00:32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>45</td>\n",
       "      <td>0.045586</td>\n",
       "      <td>0.867643</td>\n",
       "      <td>0.791217</td>\n",
       "      <td>00:33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>46</td>\n",
       "      <td>0.035273</td>\n",
       "      <td>0.838097</td>\n",
       "      <td>0.795437</td>\n",
       "      <td>00:32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>47</td>\n",
       "      <td>0.040044</td>\n",
       "      <td>0.845941</td>\n",
       "      <td>0.796062</td>\n",
       "      <td>00:32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>48</td>\n",
       "      <td>0.031059</td>\n",
       "      <td>0.822033</td>\n",
       "      <td>0.796843</td>\n",
       "      <td>00:31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>49</td>\n",
       "      <td>0.036295</td>\n",
       "      <td>0.858631</td>\n",
       "      <td>0.793952</td>\n",
       "      <td>00:33</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Better model found at epoch 0 with valid_loss value: 0.6790475249290466.\n",
      "Better model found at epoch 2 with valid_loss value: 0.6308445334434509.\n",
      "Better model found at epoch 3 with valid_loss value: 0.6153716444969177.\n",
      "Better model found at epoch 4 with valid_loss value: 0.6077684760093689.\n",
      "Better model found at epoch 6 with valid_loss value: 0.5594715476036072.\n",
      "Better model found at epoch 7 with valid_loss value: 0.5469342470169067.\n",
      "Better model found at epoch 9 with valid_loss value: 0.5352318286895752.\n",
      "Better model found at epoch 10 with valid_loss value: 0.5163002014160156.\n",
      "Better model found at epoch 12 with valid_loss value: 0.4812472462654114.\n",
      "Better model found at epoch 15 with valid_loss value: 0.47725504636764526.\n",
      "Better model found at epoch 16 with valid_loss value: 0.4745529592037201.\n",
      "Better model found at epoch 19 with valid_loss value: 0.467233270406723.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<br/>Waiting for W&B process to finish, PID 561... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\">\n",
       "<h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>accuracy</td><td>▁▁▃▃▃▅▅▅▆▆▇▆▇▇▇█▇█▇██▇▇▇▇▇██▇▇▇▇▇▇▇▇▇▇▇▇</td></tr><tr><td>epoch</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>eps_0</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>lr_0</td><td>▁▁▂▃▄▅▆▇▇██████▇▇▇▇▆▆▆▅▅▅▄▄▄▃▃▃▂▂▂▁▁▁▁▁▁</td></tr><tr><td>mom_0</td><td>██▇▆▅▄▃▂▂▁▁▁▁▁▁▂▂▂▂▃▃▃▄▄▄▅▅▅▆▆▆▇▇▇██████</td></tr><tr><td>raw_loss</td><td>█▇▆▇▅▆▇▇▅▇▅▅▅▄▄▄▄▃▃▄▃▄▄▃▃▂▂▂▁▂▁▂▁▂▁▁▁▁▁▁</td></tr><tr><td>sqr_mom_0</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_loss</td><td>█▇▇▇▆▆▆▆▆▅▅▅▅▄▄▄▄▄▃▃▃▃▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>valid_loss</td><td>▅▅▄▄▄▃▂▃▂▂▁▂▁▁▁▁▁▁▂▂▂▃▃▃▃▄▄▄▅▆▆▆▇▇▇██▇██</td></tr><tr><td>wd_0</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\">\n",
       "<h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>accuracy</td><td>0.79395</td></tr><tr><td>epoch</td><td>50</td></tr><tr><td>eps_0</td><td>1e-05</td></tr><tr><td>lr_0</td><td>0.0</td></tr><tr><td>mom_0</td><td>0.95</td></tr><tr><td>raw_loss</td><td>0.0127</td></tr><tr><td>sqr_mom_0</td><td>0.99</td></tr><tr><td>train_loss</td><td>0.0363</td></tr><tr><td>valid_loss</td><td>0.85863</td></tr><tr><td>wd_0</td><td>1e-05</td></tr></table>\n",
       "</div></div>\n",
       "Synced 6 W&B file(s), 0 media file(s), 1 artifact file(s) and 1 other file(s)\n",
       "<br/>Synced <strong style=\"color:#cdcd00\">All vars, dropout=0.1</strong>: <a href=\"https://wandb.ai/sashalikesplanes/HOClass/runs/2y7jk59p\" target=\"_blank\">https://wandb.ai/sashalikesplanes/HOClass/runs/2y7jk59p</a><br/>\n",
       "Find logs at: <code>./wandb/run-20220201_114745-2y7jk59p/logs</code><br/>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "config = AttrDict (\n",
    "    batch_size = 64,\n",
    "    nf = 24, # number of convolutional filters\n",
    "    ks = 64, # largest kernel size of convolution filter\n",
    "    bottleneck = False, # use bottleneck\n",
    "    bottleneck_size = 0,\n",
    "    depth = 6, # number of inception models, best 3\n",
    "    residual = True, # best False\n",
    "    opt_func = Adam,\n",
    "    valid_pct = 0.2,\n",
    "    wd = 5e-6, # weight decay\n",
    "    lr = 1e-3,\n",
    "    n_epochs = 50,\n",
    "    variables = ['e', 'u', 'x' 'dedt', 'dudt', 'dxdt'],\n",
    "    conv_dropout = 0.1,\n",
    "    standardize_by_var = True,\n",
    "    standardize_by_sample = True,\n",
    "    )\n",
    "\n",
    "\n",
    "with wandb.init(project=\"HOClass\", config=config, group=\"InceptionTimePlus\", name=\"All vars, dropout=0.1\", save_code=True,):\n",
    "\n",
    "    \n",
    "   "
   ]
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "conda-root-py",
   "name": "pytorch-gpu.1-10.m86",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/pytorch-gpu.1-10:m86"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
